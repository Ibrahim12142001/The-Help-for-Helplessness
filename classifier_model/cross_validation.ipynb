{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c96ca53",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a3ad2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from cnn_2d_model.CNN_LSTM_model import HelplessnessClassifier as CNN_LSTM_Classifier\n",
    "from cnn_3d_model.model import HelplessnessClassifier as CNN_3D_Classifier\n",
    "from pre_trained_transformer_model.model import create_swin3d_t_model_training\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps' # Apple Silicon \n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' # Nvidia GPU\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ea243",
   "metadata": {},
   "source": [
    "# Custom dataset for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c15c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelplessnessVideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, train_transform=None, test_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "        self.train = True\n",
    "        self.video_folders = []\n",
    "\n",
    "        # Read all video sequences in the extracted frames folders\n",
    "        categories = ['extreme-helpless', 'little_helplessness', 'no-helpless']\n",
    "        for category in categories:\n",
    "            category_dir = os.path.join(root_dir, category)\n",
    "            if not os.path.exists(category_dir):\n",
    "                print(f\"Warning: Category folder {category_dir} does not exist.\")\n",
    "                continue\n",
    "            for video_folder in sorted(os.listdir(category_dir)):\n",
    "                video_path = os.path.join(category_dir, video_folder)\n",
    "                if os.path.isdir(video_path):\n",
    "                    self.video_folders.append(video_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_folders)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_folders[index]\n",
    "\n",
    "        # Each video is a sequence of frames, so need to get each frame\n",
    "        frame_files = sorted(os.listdir(video_path))\n",
    "        sequence = []\n",
    "        random_state = torch.get_rng_state()\n",
    "        for frame_name in frame_files:\n",
    "            frame_path = os.path.join(video_path, frame_name)\n",
    "            frame = Image.open(frame_path)\n",
    "            if self.train_transform and self.test_transform:\n",
    "                # To allow augmentation, we need to apply the same \"random\" transformation to each frame\n",
    "                torch.set_rng_state(random_state)\n",
    "                frame = self.train_transform(frame) if self.train else self.test_transform(frame)\n",
    "            sequence.append(frame)\n",
    "\n",
    "        # Convert the sequence from list of tensors to (sequence_length, channels, height, width) tensor\n",
    "        sequence = torch.stack(sequence)\n",
    "        sequence = torch.transpose(sequence, 0, 1)  # REMOVE THIS IF YOU NEED THE SEQUENCE_LENGTH AND CHANNELS DIMENSIONS SWITCHED!\n",
    "\n",
    "        # Retrieve the level of helplessness label from path of video\n",
    "        split_path = video_path.split(os.sep)  # Changed '/' to os.sep for cross-platform compatibility\n",
    "\n",
    "        # Ensure that split_path has at least 2 components\n",
    "        if len(split_path) >= 2:\n",
    "            category = split_path[-2]  # category folder is second last in path\n",
    "        else:\n",
    "            raise ValueError(f\"Path structure issue: {video_path}\")\n",
    "\n",
    "        label = -1\n",
    "        if category == 'no-helpless':\n",
    "            label = 0\n",
    "        elif category == 'little_helplessness':\n",
    "            label = 1\n",
    "        elif category == 'extreme-helpless':\n",
    "            label = 2\n",
    "\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5590c8",
   "metadata": {},
   "source": [
    "# 2D CNN-LSTM 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a5a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transforms and dataset for 2D CNN-LSTM\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((112, 112), scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),  # converts PIL Image in L mode to tensor shape (1, H, W) with pixel values [0,1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = HelplessnessVideoDataset('../data/train', train_transform=train_transform, test_transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737a9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------\n",
      "[DEBUG] CNN feature_dim = 25088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 32980, 28852) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\gregp\\miniconda3\\envs\\cmpt419project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\gregp\\miniconda3\\envs\\cmpt419project\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     37\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# ensure train transforms on used on the data\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# need to transpose the sequence length and channel dimensions\u001b[39;49;00m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msequences\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gregp\\miniconda3\\envs\\cmpt419project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\gregp\\miniconda3\\envs\\cmpt419project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gregp\\miniconda3\\envs\\cmpt419project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\gregp\\miniconda3\\envs\\cmpt419project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 32980, 28852) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# to do 5-fold cross validation, we are using the approach shown here: https://saturncloud.io/blog/how-to-use-kfold-cross-validation-with-dataloaders-in-pytorch/\n",
    "\n",
    "# Initialize how many folds we want for cross-validation\n",
    "# Using KFold from scikit-learn for this: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "average_f1score = 0\n",
    "\n",
    "# Loop through each fold, training the model and reporting results\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('------')\n",
    "\n",
    "    # Create data loaders for the current fold\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_index),\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=4,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_index),\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize the model, loss criterion, optimizer and number of epoches\n",
    "    model = CNN_LSTM_Classifier(num_classes=3).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    num_epochs = 20\n",
    "\n",
    "    # Train the model for the given epoches on the train_loader for this fold\n",
    "    for epochs in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_dataset.train = True # ensure train transforms on used on the data\n",
    "\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "            sequences = torch.transpose(sequences, 1, 2) # need to transpose the sequence length and channel dimensions\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize the model\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test_loader for this fold\n",
    "    model.eval()\n",
    "    train_dataset.train = False # ensure test transforms on used on the data\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (sequences, labels) in enumerate(test_loader):\n",
    "            sequences = torch.transpose(sequences, 1, 2) # need to transpose the sequence length and channel dimensions\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Store predictions and ground truth labels in lists\n",
    "            predictions.extend(predicted.detach().cpu().tolist())\n",
    "            ground_truth.extend(labels.detach().cpu().tolist())\n",
    "    \n",
    "    # Using scikit-learn, print the F1 score of this fold: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    from sklearn.metrics import f1_score\n",
    "    fold_f1score = f1_score(ground_truth, predictions, average='weighted') * 100\n",
    "    fold_accuracy = 100 * correct / total\n",
    "    print(f'Fold F1 Score: {fold_f1score:.2f}%, Fold Accuracy: {fold_accuracy:.2f}%\\n')\n",
    "    average_f1score += fold_f1score\n",
    "\n",
    "average_f1score /= 5\n",
    "print(f'Average F1 Score across all folds: {average_f1score:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmpt419project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
